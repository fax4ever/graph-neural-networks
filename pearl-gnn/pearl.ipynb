{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b22e38c",
   "metadata": {},
   "source": [
    "# PEARL reimplementation\n",
    "\n",
    "@misc{kanatsoulis2025learningefficientpositionalencodings,\n",
    "      title={Learning Efficient Positional Encodings with Graph Neural Networks}, \n",
    "      author={Charilaos I. Kanatsoulis and Evelyn Choi and Stephanie Jegelka and Jure Leskovec and Alejandro Ribeiro},\n",
    "      year={2025},\n",
    "      eprint={2502.01122},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.LG},\n",
    "      url={https://arxiv.org/abs/2502.01122}, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a320c803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fax/.conda/envs/pearl-pe/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random, numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41cac0",
   "metadata": {},
   "source": [
    "## Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7724f604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Device name: AMD Radeon RX 7800 XT\n",
      "Using ZINC subset (12K)\n",
      "Mode: B-PEARL\n"
     ]
    }
   ],
   "source": [
    "class HyperParam:\n",
    "    # Dataset properties (ZINimport torch.nn as nn specific)\n",
    "    n_node_types: int = 28  # Number of atom types in ZINC\n",
    "    n_edge_types: int = 3   # Number of bond types in ZINC\n",
    "\n",
    "    # General hyperparameters\n",
    "    seed: int = 42\n",
    "    device: torch.device\n",
    "    device_name: str\n",
    "\n",
    "    # Model > MLP hyperparameters\n",
    "    n_mlp_layers: int = 3\n",
    "    mlp_hidden_dims: int = 128\n",
    "    mlp_dropout_prob: float = 0.0\n",
    "    mlp_norm_type: str = \"batch\"\n",
    "\n",
    "    # Model > GINE hyperparameters\n",
    "    n_base_layers: int = 4\n",
    "    node_emb_dims: int = 128\n",
    "    base_hidden_dims: int = 128\n",
    "    gine_model_bn: bool = False\n",
    "    pooling: str = \"add\"\n",
    "    target_dim: int = 1\n",
    "\n",
    "    # Model > GIN / SampleAggregator hyperparameters\n",
    "    gin_sample_aggregator_bn: bool = True\n",
    "    n_sample_aggr_layers: int = 8\n",
    "    sample_aggr_hidden_dims: int = 40\n",
    "\n",
    "    # Model > Positional Encoding / PEARL\n",
    "    pe_dims: int = 37  # Based on SPE paper (Huang et al., 2023)\n",
    "    basis: bool = True  # False for R-PEARL, True for B-PEARL\n",
    "    num_samples: int = 120  # Number of samples for R-PEARL\n",
    "    pearl_k: int = 7  # Polynomial filter order\n",
    "    pearl_mlp_nlayers: int = 1\n",
    "    pearl_mlp_hid: int = 37\n",
    "    pearl_mlp_out: int = 37\n",
    "\n",
    "    # Dataset hyperparameters\n",
    "    use_subset: bool = True  # Use ZINC subset (12K graphs) or full (250K)\n",
    "    train_batch_size: int = 32\n",
    "    val_batch_size: int = 32\n",
    "    test_batch_size: int = 32\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-6\n",
    "    num_epochs: int = 10  # Set to 1400 for full training (as in paper)\n",
    "    n_warmup_steps: int = 100\n",
    "    \n",
    "    def __init__(self):\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device(\"mps\")\n",
    "            self.device_name = \"MPS\"\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda\")\n",
    "            self.device_name = torch.cuda.get_device_name(0)\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            self.device_name = \"CPU\"\n",
    "        self.set_seed()\n",
    "\n",
    "    def set_seed(self) -> None:\n",
    "        \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "        torch.cuda.manual_seed_all(self.seed)\n",
    "\n",
    "    def device(self) -> torch.device:\n",
    "        return self.device    \n",
    "\n",
    "# Initialize hyperparameters\n",
    "hp = HyperParam()\n",
    "print(f\"Device: {hp.device}\")\n",
    "print(f\"Device name: {hp.device_name}\")\n",
    "print(f\"Using {'ZINC subset (12K)' if hp.use_subset else 'Full ZINC (250K)'}\")\n",
    "print(f\"Mode: {'R-PEARL' if not hp.basis else 'B-PEARL'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a659fb",
   "metadata": {},
   "source": [
    "## General MPL\n",
    "\n",
    "We're going to apply on top of different models:\n",
    "* A \n",
    "* B\n",
    "* C\n",
    "\n",
    "With the possibility to support masking. In this case of course the shape of the mask should be compatinle with the shape of the variable `X` forwared to the model.\n",
    "\n",
    "* for each layers:\n",
    "  1. Linar Layer (linear projection)\n",
    "  2. Apply masking (if present)\n",
    "  3. Normalization (batch or layer)\n",
    "  4. Activation function (usually the Relu)\n",
    "  5. Dropout (if configured)\n",
    "\n",
    "* at the end:\n",
    "  1. Final Linar Layer (linear projection)\n",
    "  2. Dropout (if configured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c413da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLayer(nn.Module):\n",
    "    def __init__(self, in_dims: int, out_dims: int, hp: HyperParam):\n",
    "        super(MLPLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_dims, out_dims)\n",
    "        self.normalization = nn.BatchNorm1d(out_dims) if hp.mlp_norm_type == \"batch\" else nn.LayerNorm(out_dims)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=hp.mlp_dropout_prob)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        X = self.linear(X)\n",
    "        if mask is not None:\n",
    "            X[~mask] = 0\n",
    "\n",
    "        if mask is None:\n",
    "            shape = X.size()\n",
    "            X = X.reshape(-1, shape[-1])\n",
    "            X = self.normalization(X)\n",
    "            X = X.reshape(shape)\n",
    "        else:\n",
    "            X[mask] = self.normalization(X[mask])\n",
    "\n",
    "        X = self.activation(X)\n",
    "        X = self.dropout(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dims: int, out_dims: int, hp: HyperParam):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(hp.n_mlp_layers - 1):\n",
    "            self.layers.append(MLPLayer(in_dims, hp.mlp_hidden_dims, hp))\n",
    "            in_dims = hp.mlp_hidden_dims\n",
    "        self.linear = nn.Linear(hp.mlp_hidden_dims, out_dims)\n",
    "        self.dropout = nn.Dropout(p=hp.mlp_dropout_prob)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            X = layer(X, mask=mask)\n",
    "        X = self.linear(X)\n",
    "        X = self.dropout(X)\n",
    "        return X\n",
    "\n",
    "    @property\n",
    "    def out_dims(self) -> int:\n",
    "        return self.linear.out_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a041726",
   "metadata": {},
   "source": [
    "## GIN Model\n",
    "\n",
    "This is used as final layer of the sample aggregator model that is used to produce the positional embeddings.\n",
    "We don't have in input edge embeddings, so a GIN is enough and we don't need in this case a GINE.\n",
    "\n",
    "**NOTE** `node_dim=0` is crucial in our configuration. Since the dimensions of the expected forwarded input `X` are:\n",
    "\n",
    "0. Dimension that identifies the node\n",
    "1. Intermediate dimention: used to process in parallel (M or N) initial independent node attributes\n",
    "2. Node attributes\n",
    "\n",
    "After the message passing implemented as GIN Layer a batch normalization is applied.\n",
    "While the residual connection is not applied in the paper code base in case of ZINC.\n",
    "Actually I don't know why, since it seems that residual connections never hurt.\n",
    "Also I don't know why for the ZINC the paper implementation uses batch normalization insted of layer normalization.\n",
    "Moder tranformers use extensivly layer normalization and residual connections.\n",
    "In any case I keep my reimplementation consistent with the paper hyperpameter choice for ZINC.\n",
    "\n",
    "Last comment here is about the masking. Masking is used by the B-PEARL implementation, not by R-PEARL.\n",
    "We want to support both the kinds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf96d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): MLPLayer(\n",
       "    (linear): Linear(in_features=16, out_features=128, bias=True)\n",
       "    (normalization): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (1): MLPLayer(\n",
       "    (linear): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (normalization): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GINLayer(MessagePassing):\n",
    "    def __init__(self, in_dims: int, out_dims: int, hp: HyperParam):\n",
    "        super(GINLayer, self).__init__(aggr=\"add\", node_dim=0)\n",
    "        self.eps = nn.Parameter(data=torch.randn(1))\n",
    "        self.mlp = MLP(in_dims, out_dims, hp)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, edge_index: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        S = self.propagate(edge_index, X=X)\n",
    "        Z = (1 + self.eps) * X + S\n",
    "        return self.mlp(Z, mask=mask)\n",
    "    \n",
    "    def message(self, X_j: torch.Tensor) -> torch.Tensor:\n",
    "        return X_j\n",
    "\n",
    "    @property\n",
    "    def out_dims(self) -> int:\n",
    "        return self.mlp.out_dims\n",
    "\n",
    "\n",
    "class GIN(nn.Module):\n",
    "    def __init__(self, hp: HyperParam, residual: bool = False):\n",
    "        super(GIN, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList() if hp.gin_sample_aggregator_bn else None\n",
    "        self.residual = residual\n",
    "        \n",
    "        in_dims = hp.pearl_mlp_out\n",
    "        for _ in range(hp.n_sample_aggr_layers - 1):\n",
    "            self.layers.append(GINLayer(in_dims, hp.sample_aggr_hidden_dims, hp))\n",
    "            in_dims = hp.sample_aggr_hidden_dims\n",
    "            if self.batch_norms is not None:\n",
    "                self.batch_norms.append(nn.BatchNorm1d(hp.sample_aggr_hidden_dims))\n",
    "\n",
    "        self.layers.append(GINLayer(hp.sample_aggr_hidden_dims, hp.pe_dims, hp))\n",
    "\n",
    "    def forward(self, X: torch.Tensor, edge_index: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            X0 = X\n",
    "            X = layer(X, edge_index, mask=mask)\n",
    "            if mask is not None:\n",
    "                X[~mask] = 0\n",
    "            if self.batch_norms is not None and i < len(self.layers) - 1:\n",
    "                if mask is None:\n",
    "                    X = self.batch_norms[i](X.transpose(2, 1)).transpose(2, 1) if X.ndim == 3 else self.batch_norms[i](X)\n",
    "                else:address\n",
    "                    X[mask] = self.batch_norms[i](X[mask])\n",
    "            if self.residual:\n",
    "                X = X + X0\n",
    "        return X\n",
    "\n",
    "    @property\n",
    "    def out_dims(self) -> int:\n",
    "        return self.layers[-1].out_dims\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pearl-pe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
